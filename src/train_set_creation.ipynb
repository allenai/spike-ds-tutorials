{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e6e2dd",
   "metadata": {},
   "source": [
    "# Data Validation\n",
    "\n",
    "After getting sentences that match our patterns using SPIKE's API, we would want to make sure the following hold, before tagging the data.\n",
    "\n",
    "1. Sentences are not too short (like titles).\n",
    "2. Captures make sense ( no non-alphabetical results etc.)\n",
    "3. Spike is captures-oriented, that is, it returns a match per set of capture. We'd like to merge matches that are the same sentence that is because it has more than a single capture - for example\n",
    "  a. sent 1: [David Bowie] and Freddie Mercury\n",
    "  b. sent 2: David Bowie and [Freddie Mercury]\n",
    "This should be merge that both are labeled with the musician label.\n",
    "5. Similarly for non-musicians we'd like to ignore the captures and just look at the NER results.\n",
    "6. `'s` is not part of the entity\n",
    "7. Sentences in the train set do not appear in the test/dev sets.\n",
    "\n",
    "and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08d24f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import jsonlines\n",
    "from random import sample, shuffle\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bcd18c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = 'positive'\n",
    "DATAPATH = '../data'\n",
    "DATASET_PATH = f'{DATAPATH}/musicians_dataset'\n",
    "SPIKE_MATCH_PATH = f'{DATAPATH}/spike_matches'\n",
    "VERSION_NAME = \"train_only_hearst_uniques\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4549c6bb",
   "metadata": {},
   "source": [
    "### Extract dev/test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbf31db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(sentence):\n",
    "    tokens = []\n",
    "    for t in sentence.split():\n",
    "        if t:\n",
    "            tokens.append(t.split('-[',1)[0])\n",
    "    return clean_punct(\" \".join(tokens))\n",
    "\n",
    "def clean_punct(sentence):\n",
    "    s = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    s = s.replace(\"  \", \" \")\n",
    "    return s\n",
    "\n",
    "def get_dev_and_test_sentences(dataset_path):\n",
    "    test_path = dataset_path + '/test.txt'\n",
    "    dev_path = dataset_path + '/dev.txt'\n",
    "    with open(test_path, 'r') as ft, open(dev_path, 'r') as fd:\n",
    "        test_set = [remove_tags(sent.strip()) for sent in ft.readlines()]\n",
    "        dev_set = [remove_tags(sent.strip()) for sent in fd.readlines()]\n",
    "    dev_and_test = dev_set + test_set\n",
    "    return dev_and_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf21cdd",
   "metadata": {},
   "source": [
    "### validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c99d2ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validations\n",
    "def sentence_is_not_too_short(sentence_text):\n",
    "    return len(sentence_text) > 50\n",
    "\n",
    "def capture_is_not_non_alphabetical(capture_text):\n",
    "    alphabet = 'q w e r t y u i o p a s d f g h j k l z x c v b n m'.split()\n",
    "    return any(x in capture_text for x in alphabet)\n",
    "\n",
    "\n",
    "def validate_sentence(sentence_dict, label, capture_text, sentence_text, dev_and_test):\n",
    "    if not capture_is_not_non_alphabetical(capture_text): \n",
    "        return False\n",
    "    if not sentence_is_not_too_short(sentence_text): \n",
    "        return False\n",
    "    if sentence_text in dev_and_test: \n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b412b22",
   "metadata": {},
   "source": [
    "### Collect sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17f561bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_capture(sentence, label):\n",
    "    tokens = sentence[\"words\"]\n",
    "    capture = sentence['captures'].get(label)\n",
    "    if capture:\n",
    "        first = capture['first'] \n",
    "        last = capture['last']\n",
    "        capture_tokens = [t for i, t in enumerate(tokens) if first <= i <= last ]\n",
    "        return \" \".join(capture_tokens), first, last\n",
    "    else:\n",
    "        return \"\", -1, -1\n",
    "\n",
    "def get_entities(sentence, cap_first, cap_last):\n",
    "    entities = set()\n",
    "    for e in sentence['entities']:\n",
    "        all_entity_indices = [*range(e['first'], e['last'])]\n",
    "        if all(x not in all_entity_indices for x in [cap_first, cap_last]):\n",
    "            entities.add((e['first'], e['last']))\n",
    "    return entities\n",
    "\n",
    "\n",
    "def collect_train_set_sentences():\n",
    "    train_set = dict()\n",
    "    dev_and_test = get_dev_and_test_sentences(DATASET_PATH)\n",
    "    invalids = 0\n",
    "    same_sent = 0\n",
    "    for file in glob.glob(f'{SPIKE_MATCH_PATH}/**/unique_*.jsonl', recursive=True):\n",
    "        with jsonlines.open(file, \"r\") as f:\n",
    "            for sentence_dict in f:\n",
    "                label = LABEL if 'positive' in file else 'negative'\n",
    "                sentence_text = clean_punct(\" \".join(sentence_dict[\"words\"])).strip()\n",
    "                capture_text, cap_first, cap_last = get_capture(sentence_dict, label)\n",
    "                if capture_text:\n",
    "                    if not validate_sentence(sentence_dict, label, capture_text, sentence_text, dev_and_test):\n",
    "                        invalids += 1\n",
    "                        continue\n",
    "                    if sentence_text not in train_set.keys():\n",
    "                        if label == 'positive':\n",
    "                            train_set[sentence_text] = {\n",
    "                                \"id\": sentence_dict[\"sentence_index\"],\n",
    "                                \"label\": label,\n",
    "                                \"words\": sentence_dict[\"words\"],\n",
    "                                \"captures\": {(cap_first, cap_last)},\n",
    "                                \"entities\": get_entities(sentence_dict, cap_first, cap_last),\n",
    "                                \"need_tagging\": True\n",
    "                            }\n",
    "                        else:\n",
    "                            entities = get_entities(sentence_dict, cap_first, cap_last)\n",
    "                            entities.update({(cap_first, cap_last)})\n",
    "                            train_set[sentence_text] = {\n",
    "                                \"id\": sentence_dict[\"sentence_index\"],\n",
    "                                \"label\": label,\n",
    "                                \"words\": sentence_dict[\"words\"],\n",
    "                                \"captures\": {},\n",
    "                                \"entities\": entities,\n",
    "                                \"need_tagging\": True\n",
    "                            }    \n",
    "                    else:\n",
    "                        if label == 'positive':\n",
    "                            train_set[sentence_text][\"captures\"].add((cap_first, cap_last))\n",
    "                            new_entities = get_entities(sentence_dict, cap_first, cap_last)\n",
    "                            train_set[sentence_text][\"entities\"].update(new_entities)\n",
    "                        elif (cap_first, cap_last) not in train_set[sentence_text][\"captures\"]:\n",
    "                            train_set[sentence_text][\"entities\"].add((cap_first, cap_last))\n",
    "                        else:\n",
    "                            # not a true negative!\n",
    "                            continue\n",
    "                else:\n",
    "                    if sentence_text not in train_set:\n",
    "                        train_set[sentence_text] = {\n",
    "                            \"id\": sentence_dict[\"sentence_index\"],\n",
    "                            \"label\": label,\n",
    "                            \"words\": sentence_dict[\"words\"],\n",
    "                            \"captures\": {},\n",
    "                            \"entities\": {},\n",
    "                            \"need_tagging\": False\n",
    "                        }\n",
    "                    else:\n",
    "                        same_sent += 1\n",
    "                 \n",
    "    \n",
    "    # make sure there are significantly more negative examples than positive ones. \n",
    "    print(\"Number of negatives: \", len([x for x, y in train_set.items() if y[\"label\"] != 'positive']))\n",
    "    print(\"Number of positives: \", len([x for x, y in train_set.items() if y[\"label\"] == 'positive']))\n",
    "    print(\"invalids: \", invalids)\n",
    "    print(\"same_sent: \", same_sent)\n",
    "    \n",
    "    return train_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731efedd",
   "metadata": {},
   "source": [
    "## Tag Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ae84e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, save as one token per line\n",
    "def flatten_list(ent_list):\n",
    "    return [item for sublist in ent_list for item in sublist]\n",
    "    \n",
    "\n",
    "def tag_sentence_one_token_per_row(sentence):\n",
    "    if sentence[\"need_tagging\"]:\n",
    "        tags = []\n",
    "        captures = [[*range(span[0], span[1]+1)] for span in sentence[\"captures\"]]\n",
    "        entities = [[*range(span[0], span[1]+1)] for span in sentence[\"entities\"]]\n",
    "        flat_captures = flatten_list(captures)\n",
    "        flat_entities = flatten_list(entities)\n",
    "\n",
    "        for i, word in enumerate(sentence['words']):\n",
    "            if word != \"'s\":\n",
    "                if i in flat_captures:\n",
    "                    captures, tags = tag_span(captures, i, word, 'MUS', tags)\n",
    "                elif i in flat_entities:\n",
    "                    entities, tags = tag_span(entities, i, word, 'PER', tags)\n",
    "                else:\n",
    "                    tags.append((word,\"O\"))\n",
    "            else:\n",
    "                tags.append((word,\"O\"))\n",
    "        return tags\n",
    "    else:\n",
    "        return [(word, \"O\") for word in sentence['words']]\n",
    "\n",
    "\n",
    "def tag_span(span_list, i, word, tag_suffix, tags):\n",
    "    for span in span_list:\n",
    "        if i == span[0]:\n",
    "            tags.append((word, f\"B-{tag_suffix}\"))\n",
    "        elif i in span:\n",
    "            tags.append((word,f\"I-{tag_suffix}\"))\n",
    "        elif i == span[-1]:\n",
    "            tags.append((word,f\"I-{tag_suffix}\"))\n",
    "            span_list.remove(cap)\n",
    "    return span_list, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea429c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negatives:  12350\n",
      "Number of positives:  7066\n",
      "invalids:  2548\n",
      "same_sent:  187\n"
     ]
    }
   ],
   "source": [
    "train_set = collect_train_set_sentences()\n",
    "\n",
    "with jsonlines.open(f'{DATASET_PATH}/{VERSION_NAME}.jsonl', 'w') as f:\n",
    "    for sent in sample([v for v in train_set.values()], len(train_set)):\n",
    "        tags = tag_sentence_one_token_per_row(sent)\n",
    "        sent_json = {\"id\": sent[\"id\"], \"sent_items\": tags}\n",
    "        f.write(sent_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9c4ae",
   "metadata": {},
   "source": [
    "## Split Sets\n",
    "\n",
    "Create dev and test sets by splitting the current train set. The code below splits the current train set into 80%-10%-10% for train dev and test sets respectively "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f9afad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_dev_test(fp, sample=False):\n",
    "    with open(fp, \"r\") as f:\n",
    "        all_lines = f.readlines()\n",
    "        shuffle(all_lines)\n",
    "        datasize = len(all_lines)\n",
    "        dev_border = int(datasize*0.1) if not sample else 300\n",
    "        test_border = int(datasize*0.9) if not sample else datasize-300\n",
    "        with open(fp.replace(\"train\", \"split_dev\"), \"w\") as f:\n",
    "            for line in all_lines[0:dev_border]:\n",
    "                f.write(line)\n",
    "        with open(fp.replace(\"train\", \"split_train\"), \"w\") as f:\n",
    "            for line in all_lines[dev_border:test_border]:\n",
    "                f.write(line)\n",
    "        with open(fp.replace(\"train\", \"split_test\"), \"w\") as f:\n",
    "            for line in all_lines[test_border:]:\n",
    "                f.write(line)\n",
    "    \n",
    "split_train_dev_test(f'{DATASET_PATH}/{VERSION_NAME}.jsonl', sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e68ce55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.5M\t../data/musicians_dataset/split_train_only_hearst_uniques.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$DATASET_PATH\" \"$VERSION_NAME\"\n",
    "du -sh $1/split_$2.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08fd381",
   "metadata": {},
   "source": [
    "## convert dumps to pandas DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5fbcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_jsons_to_df(fp):\n",
    "    tagged_data = []\n",
    "    with jsonlines.open(fp, 'r') as f:\n",
    "        for line in f:\n",
    "            for token in line[\"sent_items\"]:\n",
    "                tagged_data.append([line[\"id\"], token[0], token[1]])\n",
    "    return pd.DataFrame(tagged_data, columns=[\"sentence_id\", \"words\", \"labels\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b210cb",
   "metadata": {},
   "source": [
    "## Inspect Dev Set\n",
    "\n",
    "Here are some methods that can help us detect bad sentences in the dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6affa65d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = import_jsons_to_df(f\"{DATASET_PATH}/sample_dev.jsonl\")\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd0f44b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_mus = df[df[\"labels\"] == \"B-MUS\"][\"sentence_id\"].unique().tolist()[1]\n",
    "df[df[\"sentence_id\"] == first_mus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3204764d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "non_alpha_persons = [x for x in df[df[\"labels\"] == \"I-PER\"][\"words\"].unique().tolist() if not x.isalpha()]\n",
    "# this c\n",
    "sentences_with_non_alpha_persons = df[df[\"words\"].isin(non_alpha_persons)]\n",
    "for i, row in sentences_with_non_alpha_persons.iterrows():\n",
    "    print(row[\"sentence_id\"], row[\"words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8342210d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_musicians = df[df[\"labels\"] == \"B-MUS\"][\"words\"].unique().tolist()\n",
    "all_musicians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac3d9b2",
   "metadata": {},
   "source": [
    "### Print Dev sentences with PER and MUS highlighted\n",
    "Another way of reading the sentences comfortably"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfc6f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(f\"{DATASET_PATH}/split_dev_only_hearst.jsonl\", 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        color_text = [x[0] if x[1] == \"O\" else colored(x[0], 'red') if \"PER\" in x[1] else colored(x[0], 'green') for x in line[\"sent_items\"]]\n",
    "        print(i, line[\"id\"], \" \".join(color_text))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8b44eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(f\"{DATASET_PATH}/dev_converted.jsonl\", 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        color_text = [x[0] if x[1] == \"O\" else colored(x[0], 'red') if \"PER\" in x[1] else colored(x[0], 'green') for x in line[\"sent_items\"]]\n",
    "        print(i, line[\"id\"], \" \".join(color_text))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f92327b",
   "metadata": {},
   "source": [
    "### Convert Dev Set format\n",
    "When collecting data using the `tag_sentences` notebook, we created sentences in the following format:\n",
    "```\n",
    "The-[O] General-[O] --[O] Director-[O] of-[O] Rustavi-[O] 2-[O]\n",
    "```\n",
    "We would like to convert the manually curated dev and test sets to the same format like the jsonlines train set.\n",
    "```\n",
    "{\"id\": 15731, \"sent_items\": [[\"His\", \"O\"], [\"wife\", \"O\"], [\"Elizabeth\", \"B-PER\"], [\"died\", \"O\"], ...]}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b6f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = {\"B\": \"B-MUS\", \"PB\": \"B-PER\", \"I\": \"I-MUS\", \"PI\": \"I-PER\", \"O\": \"O\"}\n",
    "\n",
    "def convert_format_to_jsonl(fp):\n",
    "    \"\"\"\n",
    "    input: The-[O] General-[O] --[O] Director-[O] of-[O] Rustavi-[O] 2-[O] ...\n",
    "    output : {\"id\": 15731, \"sent_items\": [[\"His\", \"O\"], [\"wife\", \"O\"], [\"Elizabeth\", \"B-PER\"], [\"died\", \"O\"], ...]}\n",
    "    \n",
    "    \"\"\"\n",
    "    def convert_token(token):\n",
    "        t, l = token.split(\"-[\")\n",
    "        l = LABELS[l[:-1]]\n",
    "        return [t, l]\n",
    "        \n",
    "    with open(fp, \"r\") as fin:\n",
    "        with jsonlines.open(f\"{fp.replace('.txt', '_converted.jsonl')}\", \"w\") as fout:\n",
    "            for i, line in enumerate(fin.readlines()):\n",
    "                line = line.split()\n",
    "                sent_items = [convert_token(x) for x in line]\n",
    "                fout.write({\"id\": i, \"sent_items\": sent_items})\n",
    "\n",
    "convert_format_to_jsonl(\"../data/musicians_dataset/test.txt\")\n",
    "convert_format_to_jsonl(\"../data/musicians_dataset/dev.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
