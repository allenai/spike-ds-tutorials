{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adc661b3",
   "metadata": {},
   "source": [
    "# Data Validation\n",
    "\n",
    "After getting sentences that match our patterns using SPIKE's API, we would want to make sure the following hold, before tagging the data.\n",
    "\n",
    "1. Sentences are not too short (like titles).\n",
    "2. Captures make sense ( no non-alphabetical results etc.)\n",
    "3. Spike is captures-oriented, that is, it returns a match per set of capture. We'd like to merge matches that are the same sentence that is because it has more than a single capture - for example\n",
    "  a. sent 1: [David Bowie] and Freddie Mercury\n",
    "  b. sent 2: David Bowie and [Freddie Mercury]\n",
    "This should be merge that both are labeled with the musician label.\n",
    "5. Similarly for non-musicians we'd like to ignore the captures and just look at the NER results.\n",
    "6. `'s` is not part of the entity\n",
    "7. Sentences in the train set do not appear in the test/dev sets.\n",
    "\n",
    "and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f3e71a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a5ec82",
   "metadata": {},
   "source": [
    "### Extract dev/test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "325d1d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(sentence):\n",
    "    tokens = []\n",
    "    for t in sentence.split():\n",
    "        if t:\n",
    "            tokens.append(t.split('-[',1)[0])\n",
    "    return clean_punct(\" \".join(tokens))\n",
    "\n",
    "def clean_punct(sentence):\n",
    "    s = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    s = s.replace(\"  \", \" \")\n",
    "    return s\n",
    "\n",
    "test_path = '../data/musicians_dataset/test.txt'\n",
    "dev_path = '../data/musicians_dataset/dev.txt'\n",
    "with open(test_path, 'r') as ft, open(dev_path, 'r') as fd:\n",
    "    test_set = [remove_tags(sent.strip()) for sent in ft.readlines()]\n",
    "    dev_set = [remove_tags(sent.strip()) for sent in fd.readlines()]\n",
    "dev_and_test = dev_set + test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2c574a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_is_not_too_short(sentence):\n",
    "    return len(sentence['words']) > 50\n",
    "\n",
    "def capture_is_not_non_alphabetical(sentence):\n",
    "    tokens = sentence[\"words\"]\n",
    "    first = sentence['captures']['musician']['first']\n",
    "    last = sentence['captures']['musician']['last']\n",
    "    capture_tokens = [t for i, t in enumerate(tokens) if first <= i <= last ]\n",
    "    alphabet = 'q w e r t y u i o p a s d f g h j k l z x c v b n m'.split()\n",
    "    return any(x in \" \".join(capture_tokens) for x in alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5aa89d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "998\n",
      "1000\n",
      "1998\n",
      "1000\n",
      "2996\n",
      "1000\n",
      "3985\n",
      "1000\n",
      "4985\n",
      "5000\n",
      "4985\n"
     ]
    }
   ],
   "source": [
    "positives = []\n",
    "negatives = []\n",
    "\n",
    "for file in glob.glob('../data/spike_matches/**/*.json', recursive=True):\n",
    "    with open(file, \"r\") as f:\n",
    "        j = json.load(f)\n",
    "        print(len(j))\n",
    "        for sent in j:\n",
    "            # start validations:\n",
    "            if sentence_is_not_too_short(sent) and capture_is_not_non_alphabetical(sent):\n",
    "                clean_sent = clean_punct(\" \".join(sent[\"words\"]))\n",
    "                if clean_sent not in dev_and_test:\n",
    "                    if 'positive' in file:\n",
    "                        positives.append(sent)\n",
    "                    else:\n",
    "                        negatives.append(sent)\n",
    "        print(len(positives))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf73ee4",
   "metadata": {},
   "source": [
    "All in all, 15 sentences from test/dev appear in the train set, and have been removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b2ea925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'words': ['Jorge',\n",
       "  'Luis',\n",
       "  'Prats',\n",
       "  'Soca',\n",
       "  '(',\n",
       "  'born',\n",
       "  '3',\n",
       "  'July',\n",
       "  '1956',\n",
       "  ')',\n",
       "  'is',\n",
       "  'a',\n",
       "  'Cuban',\n",
       "  'pianist',\n",
       "  'living',\n",
       "  'in',\n",
       "  'Spain',\n",
       "  '.'],\n",
       " 'captures': {'musician': {'first': 0, 'last': 3}},\n",
       " 'sentence_index': 4448,\n",
       " 'highlights': [{'first': 10, 'last': 10}, {'first': 13, 'last': 13}],\n",
       " 'entities': [{'first': 0,\n",
       "   'last': 3,\n",
       "   'label': 'PERSON',\n",
       "   'priority': 0,\n",
       "   'source': None}]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positives[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
