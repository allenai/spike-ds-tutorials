{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e6e2dd",
   "metadata": {},
   "source": [
    "# Data Validation\n",
    "\n",
    "After getting sentences that match our patterns using SPIKE's API, we would want to make sure the following hold, before tagging the data.\n",
    "\n",
    "1. Sentences are not too short (like titles).\n",
    "2. Captures make sense ( no non-alphabetical results etc.)\n",
    "3. Spike is captures-oriented, that is, it returns a match per set of capture. We'd like to merge matches that are the same sentence that is because it has more than a single capture - for example\n",
    "  a. sent 1: [David Bowie] and Freddie Mercury\n",
    "  b. sent 2: David Bowie and [Freddie Mercury]\n",
    "This should be merge that both are labeled with the musician label.\n",
    "5. Similarly for non-musicians we'd like to ignore the captures and just look at the NER results.\n",
    "6. `'s` is not part of the entity\n",
    "7. Sentences in the train set do not appear in the test/dev sets.\n",
    "\n",
    "and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d24f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4549c6bb",
   "metadata": {},
   "source": [
    "### Extract dev/test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf31db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(sentence):\n",
    "    tokens = []\n",
    "    for t in sentence.split():\n",
    "        if t:\n",
    "            tokens.append(t.split('-[',1)[0])\n",
    "    return clean_punct(\" \".join(tokens))\n",
    "\n",
    "def clean_punct(sentence):\n",
    "    s = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    s = s.replace(\"  \", \" \")\n",
    "    return s\n",
    "\n",
    "def get_dev_and_test_sentences(dataset_path):\n",
    "    test_path = dataset_path + '/test.txt'\n",
    "    dev_path = dataset_path + '/dev.txt'\n",
    "    with open(test_path, 'r') as ft, open(dev_path, 'r') as fd:\n",
    "        test_set = [remove_tags(sent.strip()) for sent in ft.readlines()]\n",
    "        dev_set = [remove_tags(sent.strip()) for sent in fd.readlines()]\n",
    "    dev_and_test = dev_set + test_set\n",
    "    return dev_and_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a59800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_capture(sentence, label):\n",
    "    tokens = sentence[\"words\"]\n",
    "    first = sentence['captures'][label]['first']\n",
    "    last = sentence['captures'][label]['last']\n",
    "    capture_tokens = [t for i, t in enumerate(tokens) if first <= i <= last ]\n",
    "    return \" \".join(capture_tokens), first, last\n",
    "\n",
    "def get_entities(sentence):\n",
    "    return [(e['first'], e['last']) for e in sentence['entities']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99d2ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validations\n",
    "def sentence_is_not_too_short(sentence_text):\n",
    "    return len(sentence_text) > 50\n",
    "\n",
    "def capture_is_not_non_alphabetical(capture_text):\n",
    "    alphabet = 'q w e r t y u i o p a s d f g h j k l z x c v b n m'.split()\n",
    "    return any(x in capture_text for x in alphabet)\n",
    "\n",
    "\n",
    "def validate_sentence(sentence_dict, label, capture_text, sentence_text, dev_and_test):\n",
    "    if not capture_is_not_non_alphabetical(capture_text): return None, None\n",
    "    if not sentence_is_not_too_short(sentence_text): return None, None\n",
    "    if sentence_text in dev_and_test: return None, None\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8230886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge similar sentences\n",
    "positives[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f561bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = dict()\n",
    "LABEL = 'musician'\n",
    "dev_and_test = get_dev_and_test_sentences('../data/musicians_dataset')\n",
    "\n",
    "\n",
    "for file in glob.glob('../data/spike_matches/**/*.json', recursive=True):\n",
    "    with open(file, \"r\") as f:\n",
    "        j = json.load(f)\n",
    "        for sentence_dict in j:\n",
    "            # start validations:\n",
    "            label = LABEL if 'positive' in file else 'negative'\n",
    "            capture_text, cap_first, cap_last = get_capture(sentence_dict, label)\n",
    "            entities = get_entities(sentence_dict)\n",
    "            sentence_text = clean_punct(\" \".join(sentence_dict[\"words\"])).strip()\n",
    "            if not validate_sentence(sentence_dict, label, capture_text, sentence_text, dev_and_test): continue\n",
    "            # \n",
    "            if sentence_text not in train_set.keys():\n",
    "                train_set[sentence_text] = {\n",
    "                    \"label\": label,\n",
    "                    \"words\": sentence_dict[\"words\"],\n",
    "                    \"captures\": {(cap_first, cap_last)},\n",
    "                    \"entities\": entities\n",
    "                }\n",
    "            else:\n",
    "                print(label, sentence_text)\n",
    "                if label == 'musician':\n",
    "                    \n",
    "                    train_set[sentence_text][\"captures\"].add((cap_first, cap_last))\n",
    "                else:\n",
    "                    train_set[sentence_text][\"entities\"].add((cap_first, cap_last))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec146e6f",
   "metadata": {},
   "source": [
    "All in all, 15 sentences from test/dev appear in the train set, and have been removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386afb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2893861",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"YMO are considered pioneers in the field of popular electronic music and continue to be remixed or sampled by modern artists including experimental artist Yamantaka Eye electronica group LFO jungle band 4hero electrolatino artist Senor Coconut ambient house pioneers The Orb and 808 State electronic music groups Orbital and The Human League hip hop pioneer Afrika Bambaataa and mainstream pop musicians such as Michael Jackson Quincy Jones Greg Phillinganes Eric Clapton Mariah Carey and Jennifer Lopez\"\n",
    "\n",
    "train_set[text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8685988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"YMO\", \"are\", \"considered\", \"pioneers\", \"in\", \"the\", \"field\", \"of\", \"popular\", \"electronic\", \"music\", \",\", \"and\", \"continue\", \"to\", \"be\", \"remixed\", \"or\", \"sampled\", \"by\", \"modern\", \"artists\", \",\", \"including\", \"experimental\", \"artist\", \"Yamantaka\", \"Eye\", \",\", \"electronica\", \"group\", \"LFO\", \",\", \"jungle\", \"band\", \"4hero\", \",\", \"electrolatino\", \"artist\", \"Senor\", \"Coconut\", \",\", \"ambient\", \"house\", \"pioneers\", \"The\", \"Orb\", \"and\", \"808\", \"State\", \",\", \"electronic\", \"music\", \"groups\", \"Orbital\", \"and\", \"The\", \"Human\", \"League\", \",\", \"hip\", \"hop\", \"pioneer\", \"Afrika\", \"Bambaataa\", \",\", \"and\", \"mainstream\", \"pop\", \"musicians\", \"such\", \"as\", \"Michael\", \"Jackson\", \",\", \"Quincy\", \"Jones\", \",\", \"Greg\", \"Phillinganes\", \",\", \"Eric\", \"Clapton\", \",\", \"Mariah\", \"Carey\", \",\", \"and\", \"Jennifer\", \"Lopez\", \".\"]\n",
    "words[78:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9c4adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in train_set.items():\n",
    "    if len(v[\"captures\"]) > 1:\n",
    "        print(k, v[\"captures\"], v[\"entities\"], v[\"label\"])\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
