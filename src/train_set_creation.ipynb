{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e6e2dd",
   "metadata": {},
   "source": [
    "# Data Validation\n",
    "\n",
    "After getting sentences that match our patterns using SPIKE's API, we would want to make sure the following hold, before tagging the data.\n",
    "\n",
    "1. Sentences are not too short (like titles).\n",
    "2. Captures make sense ( no non-alphabetical results etc.)\n",
    "3. Spike is captures-oriented, that is, it returns a match per set of capture. We'd like to merge matches that are the same sentence that is because it has more than a single capture - for example\n",
    "  a. sent 1: [David Bowie] and Freddie Mercury\n",
    "  b. sent 2: David Bowie and [Freddie Mercury]\n",
    "This should be merge that both are labeled with the musician label.\n",
    "5. Similarly for non-musicians we'd like to ignore the captures and just look at the NER results.\n",
    "6. `'s` is not part of the entity\n",
    "7. Sentences in the train set do not appear in the test/dev sets.\n",
    "\n",
    "and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08d24f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import jsonlines\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bcd18c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = 'musician'\n",
    "DATAPATH = '../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4549c6bb",
   "metadata": {},
   "source": [
    "### Extract dev/test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbf31db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(sentence):\n",
    "    tokens = []\n",
    "    for t in sentence.split():\n",
    "        if t:\n",
    "            tokens.append(t.split('-[',1)[0])\n",
    "    return clean_punct(\" \".join(tokens))\n",
    "\n",
    "def clean_punct(sentence):\n",
    "    s = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    s = s.replace(\"  \", \" \")\n",
    "    return s\n",
    "\n",
    "def get_dev_and_test_sentences(dataset_path):\n",
    "    test_path = dataset_path + '/test.txt'\n",
    "    dev_path = dataset_path + '/dev.txt'\n",
    "    with open(test_path, 'r') as ft, open(dev_path, 'r') as fd:\n",
    "        test_set = [remove_tags(sent.strip()) for sent in ft.readlines()]\n",
    "        dev_set = [remove_tags(sent.strip()) for sent in fd.readlines()]\n",
    "    dev_and_test = dev_set + test_set\n",
    "    return dev_and_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf21cdd",
   "metadata": {},
   "source": [
    "### validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c99d2ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validations\n",
    "def sentence_is_not_too_short(sentence_text):\n",
    "    return len(sentence_text) > 50\n",
    "\n",
    "def capture_is_not_non_alphabetical(capture_text):\n",
    "    alphabet = 'q w e r t y u i o p a s d f g h j k l z x c v b n m'.split()\n",
    "    return any(x in capture_text for x in alphabet)\n",
    "\n",
    "\n",
    "def validate_sentence(sentence_dict, label, capture_text, sentence_text, dev_and_test):\n",
    "    if not capture_is_not_non_alphabetical(capture_text): return False\n",
    "    if not sentence_is_not_too_short(sentence_text): return False\n",
    "    if sentence_text in dev_and_test: return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b412b22",
   "metadata": {},
   "source": [
    "### Collect sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17f561bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_capture(sentence, label):\n",
    "    tokens = sentence[\"words\"]\n",
    "    first = sentence['captures'][label]['first']\n",
    "    last = sentence['captures'][label]['last']\n",
    "    capture_tokens = [t for i, t in enumerate(tokens) if first <= i <= last ]\n",
    "    return \" \".join(capture_tokens), first, last\n",
    "\n",
    "def get_entities(sentence):\n",
    "    return {(e['first'], e['last']) for e in sentence['entities']}\n",
    "\n",
    "\n",
    "def collect_train_set_sentences():\n",
    "    train_set = dict()\n",
    "    dev_and_test = get_dev_and_test_sentences(f'{DATAPATH}/musicians_dataset')\n",
    "    \n",
    "    for file in glob.glob(f'{DATAPATH}/spike_matches/**/*.json', recursive=True):\n",
    "        with open(file, \"r\") as f:\n",
    "            j = json.load(f)\n",
    "            for sentence_dict in j:\n",
    "                label = LABEL if 'positive' in file else 'negative'\n",
    "                sentence_text = clean_punct(\" \".join(sentence_dict[\"words\"])).strip()\n",
    "                capture_text, cap_first, cap_last = get_capture(sentence_dict, label)\n",
    "                if not validate_sentence(sentence_dict, label, capture_text, sentence_text, dev_and_test): continue\n",
    "                if sentence_text not in train_set.keys():\n",
    "                    train_set[sentence_text] = {\n",
    "                        \"label\": label,\n",
    "                        \"words\": sentence_dict[\"words\"],\n",
    "                        \"captures\": {(cap_first, cap_last)},\n",
    "                        \"entities\": get_entities(sentence_dict) - {(cap_first, cap_last)}\n",
    "                    }\n",
    "                else:\n",
    "                    if label == 'musician':\n",
    "                        train_set[sentence_text][\"captures\"].add((cap_first, cap_last))\n",
    "                        updated_entities = train_set[sentence_text][\"entities\"] - train_set[sentence_text][\"captures\"]\n",
    "                        train_set[sentence_text].update({\"entities\": updated_entities})\n",
    "                    else:\n",
    "                        if (cap_first, cap_last) not in train_set[sentence_text][\"captures\"]:\n",
    "                            train_set[sentence_text][\"entities\"].add((cap_first, cap_last))\n",
    "                        else:\n",
    "                            # not a true negative!\n",
    "                            continue\n",
    "    \n",
    "    # make sure there are significantly more negative examples than positive ones. \n",
    "    print(\"Number of negatives: \", len([x for x, y in train_set.items() if y[\"label\"] != 'musician']))\n",
    "    print(\"Number of positives: \", len([x for x, y in train_set.items() if y[\"label\"] == 'musician']))\n",
    "    \n",
    "    return train_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731efedd",
   "metadata": {},
   "source": [
    "## Tag Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1f4b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_span(i, word, sentence, span_type, span_tagging):\n",
    "    for span in sentence[span_type]:\n",
    "        if span[0] <= i <= span[1]:\n",
    "            if i == span[0]:\n",
    "                sentence[\"tagged_sentence\"] += f\"{word}-[{span_tagging}B] \"\n",
    "            elif i == span[1]:\n",
    "                if word == \"'s\":\n",
    "                    sentence[\"tagged_sentence\"] += f\"{word}-[O] \"\n",
    "                else:\n",
    "                    sentence[\"tagged_sentence\"] += f\"{word}-[{span_tagging}I] \"\n",
    "                sentence[span_type].remove(span)                \n",
    "            else:\n",
    "                sentence[\"tagged_sentence\"] += f\"{word}-[{span_tagging}I] \"\n",
    "            return True\n",
    "\n",
    "\n",
    "def tag_sentence(sentence):\n",
    "    sentence[\"tagged_sentence\"] = \"\"\n",
    "    captures = list(sentence[\"captures\"])\n",
    "    entities = list(sentence[\"entities\"])\n",
    "    for i, word in enumerate(sentence['words']):\n",
    "        if sentence['label'] == LABEL:\n",
    "            is_capture = tag_span(i, word, sentence, 'captures', '')\n",
    "        else:\n",
    "            is_capture = tag_span(i, word, sentence, 'captures', 'P')\n",
    "        is_entity = tag_span(i, word, sentence, 'entities', 'P')\n",
    "        if not (is_capture or is_entity):\n",
    "            sentence[\"tagged_sentence\"] += f\"{word}-[O] \"        \n",
    "    sentence.update({\"entities\": entities, \"captures\": captures})\n",
    "    \n",
    "    return sentence\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4b23f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negatives:  28393\n",
      "Number of positives:  5322\n"
     ]
    }
   ],
   "source": [
    "train_set = collect_train_set_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c46ed22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{DATAPATH}/musicians_dataset/train_set_with_hearst.txt', 'w') as f:\n",
    "    for sent in sample([v for v in train_set.values()], len(train_set)):\n",
    "        tagged = tag_sentence(sent)\n",
    "        f.write(tagged[\"tagged_sentence\"] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9c4ae",
   "metadata": {},
   "source": [
    "## Alternative Sets\n",
    "\n",
    "Create other dev and test sets. The code below splits the current train set into 80%-10%-10% for train dev and test sets respectively "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7c89755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATASET = '../data/musicians_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aeb95ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_set(train_set_name):\n",
    "    trainpath = f'{DATASET}/{train_set_name}.txt'\n",
    "    df = pd.read_csv(trainpath, sep='\\t', names=['sentence'])\n",
    "    sp_train, sp_dev = train_test_split(df, test_size=0.2)\n",
    "    \n",
    "    # this is significantly slower, but ensures the output is exactly like the input, with no extra escaping and quotes.\n",
    "    with open(f'{DATASET}/split_{train_set_name}.txt', \"w\") as ft:\n",
    "        for i, row in sp_train.iterrows():\n",
    "            ft.write(row['sentence']+\"\\n\")\n",
    "    with open(f'{DATASET}/split_{train_set_name.replace(\"train\", \"dev\")}.txt', \"w\") as fd:\n",
    "        for i, row in sp_dev.iterrows():\n",
    "            fd.write(row['sentence']+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b6f51a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_train_set(\"train_set_with_hearst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440a3473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
