{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d0b28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "from simpletransformers.ner import NERModel, NERArgs\n",
    "import re\n",
    "import json\n",
    "import wandb\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd97dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_NAME = \"sents_with_no_ents\"\n",
    "EXPERIMENT_NAME = f\"{WANDB_NAME}-manual\"\n",
    "\n",
    "DATASET = '../data/musicians_dataset'\n",
    "OUTPUTS = './outputs'\n",
    "EXPERIMENTS = '../experiments'\n",
    "LABELS = {\"B\": \"B-MUS\", \"PB\": \"B-PER\", \"I\": \"I-MUS\", \"PI\": \"I-PER\", \"O\": \"O\"}\n",
    "DEV, TRAIN, TEST = \"dev_converted\", \"split_train_2\", \"split_test_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f93d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 3\n",
    "ARGS = {\n",
    "        \"seed\": 42,\n",
    "        \"labels_list\": list(LABELS.values()),\n",
    "        \"reprocess_input_data\": True,\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"train_batch_size\": BATCH_SIZE,\n",
    "        \"eval_batch_size\": BATCH_SIZE,\n",
    "        \"num_train_epochs\": EPOCHS,\n",
    "        \"save_eval_checkpoints\": False,\n",
    "        \"save_steps\": -1,\n",
    "        \"use_multiprocessing\": False,\n",
    "        \"use_multiprocessing_for_evaluation\": False,\n",
    "        \"evaluate_during_training\": True,\n",
    "        \"evaluate_during_training_steps\": 50,\n",
    "        \"evaluate_during_training_verbose\": True,\n",
    "        \"fp16\": False,\n",
    "        \"wandb_project\": WANDB_NAME,\n",
    "        \"learning_rate\": 0.0003,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"logging_steps\": 1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c7a537",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a506da8",
   "metadata": {},
   "source": [
    "### data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f281c675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data_to_df(filepath, label_map):\n",
    "    tagged_data = []\n",
    "    sentence_number = 0\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            for tagged_word in line.split():\n",
    "                try:\n",
    "                    word, tag = tagged_word.split(\"-[\", 1)\n",
    "                    old_tag = tag.split(\"]\", 1)[0]\n",
    "                    new_tag = label_map[old_tag]\n",
    "                    tagged_data.append([sentence_number, word, new_tag])\n",
    "                except Exception as e:\n",
    "                    raise Exception((tagged_word, line), e)\n",
    "            sentence_number += 1\n",
    "    return pd.DataFrame(tagged_data, columns=[\"sentence_id\", \"words\", \"labels\"])\n",
    "\n",
    "def get_dataset_parts(dataset_path, dev_name, train_name, test_name):\n",
    "    devpath, trainpath, testpath = f\"{dataset_path}/{dev_name}.txt\", f\"{dataset_path}/{train_name}.txt\", f\"{dataset_path}/{test_name}.txt\"\n",
    "    train = ingest_data_to_df(trainpath, LABELS)\n",
    "    test = ingest_data_to_df(testpath, LABELS)\n",
    "    dev = ingest_data_to_df(devpath, LABELS)\n",
    "    return dev, train, test\n",
    "\n",
    "def untag_dev_sentences(devpath):\n",
    "    output_path = f\"sentences_{devpath}\"\n",
    "    clean_sentences = []\n",
    "    with open(devpath, \"r\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            sentence = re.sub('\\-\\[[A-Z]*\\]',  '', line)\n",
    "            clean_sentences.append(sentence)\n",
    "    return clean_sentences\n",
    "\n",
    "\n",
    "# new data handling:\n",
    "def import_jsons_to_df(dataset_path, filename):\n",
    "    fp = f\"{dataset_path}/{filename}.jsonl\"\n",
    "    tagged_data = []\n",
    "    with jsonlines.open(fp, 'r') as f:\n",
    "        for line in f:\n",
    "            for token in line[\"sent_items\"]:\n",
    "                tagged_data.append([line[\"id\"], token[0], token[1]])\n",
    "    return pd.DataFrame(tagged_data, columns=[\"sentence_id\", \"words\", \"labels\"])\n",
    "\n",
    "def untagged_sentences(fp): \n",
    "    clean = []\n",
    "    with jsonlines.open(fp, \"r\") as f:\n",
    "        for line in f:\n",
    "            words = [w[0] for w in line[\"sent_items\"]]\n",
    "            clean.append(\" \".join(words))\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909bc4f5",
   "metadata": {},
   "source": [
    "### train and evaluate with simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8eba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_model(args, model_type=\"roberta\", model_name=\"roberta-base\"):\n",
    "    model = NERModel(\n",
    "        model_type, \n",
    "        model_name, \n",
    "        args=args\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def send_prediction_to_conll(predictions, experiments_path):\n",
    "    with open(f\"{experiments_path}/predictions/{EXPERIMENT_NAME}.conll\", \"w\") as f:\n",
    "        for sent in predictions:\n",
    "            for token_dict in sent:\n",
    "                for k, v in token_dict.items():\n",
    "                    f.write(f\"{k} {v}\\n\")\n",
    "\n",
    "def document_results(experiments_path, experiment_name, outputs):\n",
    "    details = dict()\n",
    "    with open(f\"{experiments_path}/{experiment_name}.json\", \"w\") as f:\n",
    "        with open(f\"{outputs}/best_model/model_args.json\",\"r\") as args:\n",
    "            details[\"model_args\"] = json.load(args)\n",
    "        with open(f\"{outputs}/best_model/config.json\",\"r\") as conf:\n",
    "            details[\"best_model_conf\"] = json.load(conf)\n",
    "        with open(f\"{outputs}/best_model/eval_results.txt\",\"r\") as res:\n",
    "            details[\"eval_results\"] = dict()\n",
    "            for r in res.readlines():\n",
    "                k, v = r.split (\" = \")\n",
    "                details[\"eval_results\"][k] = v\n",
    "        json.dump(details, f)\n",
    "\n",
    "        \n",
    "def main():\n",
    "    dev_set = import_jsons_to_df(DATASET, DEV)\n",
    "    train_set = import_jsons_to_df(DATASET, TRAIN)\n",
    "    model = configure_model(args=ARGS)\n",
    "    model.train_model(train_set, eval_data=dev_set)\n",
    "    clean_sentences = untag_dev_sentences(f\"{DATASET}/{DEV}.txt\")\n",
    "    predictions, raw_outputs = model.predict(clean_sentences)\n",
    "    document_results(EXPERIMENTS, EXPERIMENT_NAME, OUTPUTS)\n",
    "    send_prediction_to_conll(predictions, EXPERIMENTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712e01d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
